# Credit ETL Project

A complete ETL pipeline for credit application data.  
This project includes data extraction, cleaning, feature engineering, SQLite database loading, automated reports, a CLI interface, and an interactive Streamlit dashboard.

---

## Features

### **Extract**
- Load raw credit application data from CSV stored in `data/raw/`.

### **Transform**
- Data cleaning and validation
- Handling missing values
- Feature engineering:
  - `AGE` (derived from `DAYS_BIRTH`)
  - `YEARS_EMPLOYED` (derived from `DAYS_EMPLOYED`)
- Removing duplicates
- Adding a categorical target column
- Standardizing column formats

### **Load**
- Save cleaned data to:
  - CSV (`data/out/`)
  - SQLite database (`data/out/applications.db`)

### **Reports & Diagnostics**
- Correlation matrix (`data/out/correlation_matrix.jpg`)
- ETL summary text report (`data/out/etl_summary.txt`)
- Analytical SQL queries via `repositories/db.py`

### **CLI Interface**
Run database reports from the command line:

```text
ETL_Project/
│
├── main.py                 # Full ETL pipeline (extract → transform → load → diagnostics → summary)
├── dashboard.py            # Streamlit dashboard (interactive)
├── requirements.txt
├── README.md
├── .gitignore
│
├── data/
│   ├── raw/                # Raw CSV files (Kaggle dataset placed here)
│   └── out/                # Output files generated by the ETL process
│
├── models/
│   ├── cleaning.py         # Data cleaning and feature engineering
│   └── diagnostics.py      # Correlation matrix generator
│
├── repositories/
│   └── db.py               # SQLite loader + SQL analytic functions
│
├── service/
│   └── cli.py              # CLI for generating DB-based reports
│
└── utils/
    └── report.py           # ETL summary report generator
```
## Running the Project

To ensure correct file paths (especially for reading the raw CSV and writing the SQLite
database), please run all Python commands **from the project’s root directory**.

For example, if the project folder is named `ETL_Project`, run:

    cd ETL_Project
    python main.py

This guarantees that relative paths such as `data/raw/application_data.csv` and
`data/out/applications.db` resolve correctly.

The project should not be executed from inside subfolders (e.g., models/, service/, etc.).

## Understand the Dataset
https://www.kaggle.com/datasets/gauravduttakiit/loan-defaulter

application csv file:
Number of Instances: 307,511
Number of Attributes: 12 numeric/categorical variables
The explained variable is "Target" column
The dataset, sourced from the Kaggle library here and it originates from Homecredit's banking services, operational across multiple countries, including China, India, Indonesia, Vietnam, Philippines, Russia, Kazakhstan, USA, Czech Republic, and Slovakia.
Creator: Gaurav Dutta
Updated: 3 years ago
How to prepare the dataset locally

Because the full CSV file is relatively large, it is not stored directly in the GitHub repository.
Before running the project, please:

# Download the Kaggle dataset (application_data.csv) from the link above.

### Place the file as:

ETL_Project/data/raw/application_data.csv

Make sure the filename is exactly application_data.csv (or update load_raw_application() accordingly).

Once the file is in place, you can run:

python main.py
